{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecfad7fb545097e",
   "metadata": {},
   "source": [
    "# Notebook 1. Transformer Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:40.232143Z",
     "start_time": "2026-01-20T13:15:40.190200Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "651a9819e83246e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:40.252919Z",
     "start_time": "2026-01-20T13:15:40.235853Z"
    }
   },
   "source": [
    "# https://huggingface.co/google/gemma-3-1b-it\n",
    "gemma3_model_name = \"google/gemma-3-1b-it\""
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "335b1cc797f03433",
   "metadata": {},
   "source": [
    "## Exercise 1. Have a look at the Gemma-3 vocabulary and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3e890905e16115e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.259658Z",
     "start_time": "2026-01-20T13:15:40.253651Z"
    }
   },
   "source": [
    "# Load Gemma3 Tokenizer\n",
    "gemma3_tokenizer = AutoTokenizer.from_pretrained(gemma3_model_name)\n",
    "\n",
    "# Load Gemma3 Vocabulary\n",
    "gemma3_vocab = gemma3_tokenizer.get_vocab()\n",
    "\n",
    "# Convert Vocabulary to Pandas Dataframe for inspection\n",
    "gemma3_vocab_df = pd.Series(gemma3_vocab.keys(), index=gemma3_vocab.values()).sort_index()"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "317385160b0e0a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.375529Z",
     "start_time": "2026-01-20T13:15:41.336661Z"
    }
   },
   "source": [
    "# How many token types are there in the vocabulary?\n",
    "gemma3_tokenizer.vocab_size"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "9fbbf3e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.400301Z",
     "start_time": "2026-01-20T13:15:41.377488Z"
    }
   },
   "source": [
    "gemma3_vocab_df\n",
    "\n",
    "# <pad> <eos> ... are special tokens that are not part of the actual text but are used for model training.\n",
    "# <unused...> tokens are preserved for future use, e.g. fine-tuning for domain specific data."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      <pad>\n",
       "1                      <eos>\n",
       "2                      <bos>\n",
       "3                      <unk>\n",
       "4                     <mask>\n",
       "                 ...        \n",
       "262140          <unused6238>\n",
       "262141          <unused6239>\n",
       "262142          <unused6240>\n",
       "262143          <unused6241>\n",
       "262144    <image_soft_token>\n",
       "Length: 262145, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "90e3acb0b957d653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.598534Z",
     "start_time": "2026-01-20T13:15:41.424863Z"
    }
   },
   "source": [
    "# View some tokens\n",
    "gemma3_vocab_df[1000:1010]\n",
    "\n",
    "# The underscore symbol indicates a \"whitespace\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000       put\n",
       "1001       ▁di\n",
       "1002       erm\n",
       "1003    ▁about\n",
       "1004       ays\n",
       "1005      text\n",
       "1006       ▁am\n",
       "1007       ade\n",
       "1008       ▁et\n",
       "1009      ▁est\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "46d51feb627f5f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.889340Z",
     "start_time": "2026-01-20T13:15:41.871090Z"
    }
   },
   "source": [
    "# View more tokens\n",
    "gemma3_vocab_df[144620: 144630]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144620        ▁VHF\n",
       "144621       ▁Citi\n",
       "144622       ▁mede\n",
       "144623      ancang\n",
       "144624      ▁Beech\n",
       "144625    ▁Signing\n",
       "144626          你好\n",
       "144627         сай\n",
       "144628      ▁trasp\n",
       "144629      ▁varic\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "ce24933137f03120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:41.997676Z",
     "start_time": "2026-01-20T13:15:41.912845Z"
    }
   },
   "source": [
    "# Let's ask a question\n",
    "example_question = \"Jinshuai loves data science.\""
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "2cd7e49840c54542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:42.034358Z",
     "start_time": "2026-01-20T13:15:42.022418Z"
    }
   },
   "source": [
    "# Tokenize the question text and inspect the tokens we got:\n",
    "example_tokens = gemma3_tokenizer(example_question)\n",
    "\n",
    "# Convert token ids to token texts for inspection\n",
    "gemma3_tokenizer.convert_ids_to_tokens(example_tokens[\"input_ids\"])\n",
    "# <bos> marks the beginning of a sequence. Underscore \"▁\" marks \"whitespaces\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>', 'Jin', 'shu', 'ai', '▁loves', '▁data', '▁science', '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "b0a6856178c7aef3",
   "metadata": {},
   "source": [
    "## Exercise 2. Explore the output of Gemma-3 model"
   ]
  },
  {
   "cell_type": "code",
   "id": "39d4cbf7ea982e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:43.552920Z",
     "start_time": "2026-01-20T13:15:42.048588Z"
    }
   },
   "source": [
    "# Load Gemma3 Model\n",
    "gemma3_model = AutoModelForCausalLM.from_pretrained(gemma3_model_name)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "7ad8f501491769c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:43.677649Z",
     "start_time": "2026-01-20T13:15:43.651492Z"
    }
   },
   "source": [
    "# Have a look of the model\n",
    "gemma3_model\n",
    "\n",
    "# The model has a very complex structure, but we don't have to understand the very detailed implementation. Just treat it as a black box."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "fa972cd2dbad599a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:43.704602Z",
     "start_time": "2026-01-20T13:15:43.679562Z"
    }
   },
   "source": [
    "# How many parameters are there?\n",
    "sum(p.numel() for p in gemma3_model.parameters())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999885952"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "ce5d59244d14e8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:44.051123Z",
     "start_time": "2026-01-20T13:15:43.705255Z"
    }
   },
   "source": [
    "# A function to make a single run of the model\n",
    "def predict(input_ids):\n",
    "    with torch.no_grad(): # Disable Pytorch's gradient calculation\n",
    "        # Run the model for one step\n",
    "        model_output = gemma3_model(\n",
    "            input_ids=torch.tensor([input_ids], dtype=torch.long), # Convert input_id to Pytorch's Tensor format\n",
    "            attention_mask=torch.tensor([[1] * len(input_ids)], dtype=torch.long) # Attention mask. *Has no effect in this case.\n",
    "        )\n",
    "        # Note that here we only take part of the model output\n",
    "        # The actual output is a bit more complex, but we can ignore it\n",
    "        return model_output.logits[0,-1,:]\n",
    "\n",
    "example_model_output = predict(example_tokens[\"input_ids\"])\n",
    "example_model_output_df = pd.DataFrame({\n",
    "    \"token_id\": range(len(example_model_output)),\n",
    "    \"token\": gemma3_tokenizer.convert_ids_to_tokens(range(len(example_model_output))),\n",
    "    \"scores\": example_model_output,\n",
    "})"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "3f74c68387999237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:44.099800Z",
     "start_time": "2026-01-20T13:15:44.051766Z"
    }
   },
   "source": [
    "example_model_output_df.sort_values(\"scores\", ascending=False)\n",
    "\n",
    "# The output represents the model's predicted scores for each token in the vocabulary being the next token.\n",
    "\n",
    "# \"\\n\" is a new line symbol. The model thinks the next token should be two new lines."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        token_id      token     scores\n",
       "1293        1293        ▁He  18.738159\n",
       "108          108       \\n\\n  18.094341\n",
       "138          138         ▁▁  16.264725\n",
       "107          107         \\n  15.961935\n",
       "668          668        ▁he  15.702940\n",
       "...          ...        ...        ...\n",
       "250244    250244          ꗕ -14.405449\n",
       "254551    254551          􀍷 -14.405522\n",
       "214405    214405     imsuti -14.457421\n",
       "199406    199406      ▁അപകട -14.646069\n",
       "206688    206688  ▁പ്രതിഷേധ -15.275425\n",
       "\n",
       "[262144 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>1293</td>\n",
       "      <td>▁He</td>\n",
       "      <td>18.738159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>18.094341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>▁▁</td>\n",
       "      <td>16.264725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>\\n</td>\n",
       "      <td>15.961935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>668</td>\n",
       "      <td>▁he</td>\n",
       "      <td>15.702940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250244</th>\n",
       "      <td>250244</td>\n",
       "      <td>ꗕ</td>\n",
       "      <td>-14.405449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254551</th>\n",
       "      <td>254551</td>\n",
       "      <td>􀍷</td>\n",
       "      <td>-14.405522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214405</th>\n",
       "      <td>214405</td>\n",
       "      <td>imsuti</td>\n",
       "      <td>-14.457421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199406</th>\n",
       "      <td>199406</td>\n",
       "      <td>▁അപകട</td>\n",
       "      <td>-14.646069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206688</th>\n",
       "      <td>206688</td>\n",
       "      <td>▁പ്രതിഷേധ</td>\n",
       "      <td>-15.275425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262144 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "bdc9713b93bff823",
   "metadata": {},
   "source": [
    "## Exercise 3. Generate a complete answer with greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93b9988d22a966",
   "metadata": {},
   "source": [
    "Check the stop token for Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "id": "8c2da807cd56f1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:44.212156Z",
     "start_time": "2026-01-20T13:15:44.136724Z"
    }
   },
   "source": "# gemma3_model.config.eos_token_id, gemma3_tokenizer.convert_ids_to_tokens(gemma3_model.config.eos_token_id)",
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "f053c283bbea2247",
   "metadata": {},
   "source": [
    "Greedy search: always select the token with the highest scores"
   ]
  },
  {
   "cell_type": "code",
   "id": "de2805b2e0930fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:51.246006Z",
     "start_time": "2026-01-20T13:15:44.235944Z"
    }
   },
   "source": [
    "max_token = 50\n",
    "context_token_ids = example_tokens[\"input_ids\"]\n",
    "\n",
    "step = 0\n",
    "\n",
    "while step<max_token:\n",
    "\n",
    "    # Compute the token scores\n",
    "    model_step_output = predict(context_token_ids)\n",
    "\n",
    "    # Select the token with the highest probability\n",
    "    next_token_id = torch.argmax(model_step_output).item()\n",
    "\n",
    "    # Add the generated token to the context\n",
    "    context_token_ids.append(next_token_id)\n",
    "\n",
    "    # Terminate the generation process if an EOS token is generated.\n",
    "    if next_token_id in gemma3_model.config.eos_token_id:\n",
    "        print(\"Found EOS token, generation stopped.\")\n",
    "        break\n",
    "\n",
    "    # Move one step forward\n",
    "    step += 1\n",
    "    if step == max_token:\n",
    "        print(\"Max tokens reached, generation stopped.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens reached, generation stopped.\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "c31037af8ff6a400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:51.522520Z",
     "start_time": "2026-01-20T13:15:51.502602Z"
    }
   },
   "source": [
    "# Check the generated context\n",
    "context_tokens = gemma3_tokenizer.convert_ids_to_tokens(context_token_ids)\n",
    "context_tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'Jin',\n",
       " 'shu',\n",
       " 'ai',\n",
       " '▁loves',\n",
       " '▁data',\n",
       " '▁science',\n",
       " '.',\n",
       " '▁He',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁been',\n",
       " '▁working',\n",
       " '▁on',\n",
       " '▁a',\n",
       " '▁project',\n",
       " '▁to',\n",
       " '▁predict',\n",
       " '▁customer',\n",
       " '▁churn',\n",
       " '▁for',\n",
       " '▁a',\n",
       " '▁tele',\n",
       " 'communications',\n",
       " '▁company',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'He',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁using',\n",
       " '▁Python',\n",
       " '▁and',\n",
       " '▁Pandas',\n",
       " '▁to',\n",
       " '▁clean',\n",
       " '▁and',\n",
       " '▁prepare',\n",
       " '▁the',\n",
       " '▁data',\n",
       " ',',\n",
       " '▁and',\n",
       " '▁then',\n",
       " '▁he',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁using',\n",
       " '▁Sc',\n",
       " 'ikit',\n",
       " '-',\n",
       " 'learn',\n",
       " '▁to',\n",
       " '▁build',\n",
       " '▁a',\n",
       " '▁model',\n",
       " '.',\n",
       " '▁He',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "c060e35678f7d728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:15:51.550109Z",
     "start_time": "2026-01-20T13:15:51.523337Z"
    }
   },
   "source": [
    "# Put the tokens together\n",
    "print(\"\".join(context_tokens).replace(\"▁\", \" \"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Jinshuai loves data science. He's been working on a project to predict customer churn for a telecommunications company.\n",
      "\n",
      "He's using Python and Pandas to clean and prepare the data, and then he's using Scikit-learn to build a model. He'\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
